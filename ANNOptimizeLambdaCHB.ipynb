{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9IS4BXMlVtf"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# URLs crudas de los archivos CSV en el repositorio de GitHub\n",
        "train_url = \"https://raw.githubusercontent.com/gustavoleon-py/hBridgeDatasets/main/train_chb_data.csv\"\n",
        "eval_url = \"https://raw.githubusercontent.com/gustavoleon-py/hBridgeDatasets/main/eval_chb_data.csv\"\n",
        "\n",
        "# Cargar datasets desde GitHub\n",
        "try:\n",
        "    train_df = pd.read_csv(train_url)\n",
        "    eval_df = pd.read_csv(eval_url)\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar los archivos desde GitHub: {e}\")\n",
        "    print(\"Asegúrate de que los archivos 'train_chb_data.csv' y 'eval_chb_data.csv' estén en el repositorio.\")\n",
        "    exit()\n",
        "\n",
        "# Preparar datos de entrenamiento\n",
        "# Entradas: condiciones + lambda_i + lambda_v + lambda_sw (10 características)\n",
        "X_train = train_df.drop(columns=[\"THD\", \"switching_loss\"]).values\n",
        "y_train = train_df[[\"THD\", \"switching_loss\"]].values\n",
        "\n",
        "# Escalar datos para mejor entrenamiento\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "\n",
        "# Crear el modelo surrogate (predice THD y switching_loss)\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(10,)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(2)  # Salidas: THD y switching_loss\n",
        "])\n",
        "\n",
        "# Función de pérdida personalizada para entrenamiento: MSE con peso mayor en THD (2.0)\n",
        "def custom_cost_loss(y_true, y_pred):\n",
        "    error_thd = y_true[:, 0] - y_pred[:, 0]\n",
        "    weighted_error_thd = 2.0 * tf.square(error_thd)  # Mayor peso en THD\n",
        "\n",
        "    error_loss = y_true[:, 1] - y_pred[:, 1]\n",
        "    error_loss_sq = tf.square(error_loss)\n",
        "\n",
        "    # Regularización: penaliza predicciones negativas (THD y switching_loss >=0)\n",
        "    reg_thd = tf.reduce_mean(tf.maximum(0.0, -y_pred[:, 0]))\n",
        "    reg_loss = tf.reduce_mean(tf.maximum(0.0, -y_pred[:, 1]))\n",
        "\n",
        "    mse = tf.reduce_mean(weighted_error_thd + error_loss_sq)\n",
        "    reg = 0.1 * (reg_thd + reg_loss)\n",
        "\n",
        "    return mse + reg\n",
        "\n",
        "# Compilar y entrenar\n",
        "model.compile(optimizer='adam', loss=custom_cost_loss)\n",
        "model.fit(X_train_scaled, y_train_scaled, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Función para optimizar lambdas para condiciones fijas\n",
        "def optimize_lambdas(fixed_conditions, model, scaler_X, scaler_y, learning_rate=0.01, steps=200):\n",
        "    lambdas = tf.Variable([2.5, 2.5, 2.5], dtype=tf.float32)  # Inicializar lambda_i, lambda_v, lambda_sw\n",
        "    optimizer = tf.optimizers.Adam(learning_rate)\n",
        "\n",
        "    for _ in range(steps):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Construir input completo: condiciones + lambdas\n",
        "            input_full = tf.concat([fixed_conditions, lambdas], axis=0)\n",
        "            # Ensure scaling is done within the tape or with tf operations\n",
        "            # Convert scaled data back to tensor before passing to the model\n",
        "            input_scaled = scaler_X.transform(input_full.numpy().reshape(1, -1))\n",
        "            input_tensor = tf.convert_to_tensor(input_scaled, dtype=tf.float32)\n",
        "\n",
        "            # Predecir THD y switching_loss\n",
        "            pred_scaled = model(input_tensor)\n",
        "            pred = scaler_y.inverse_transform(pred_scaled.numpy())\n",
        "            pred_thd, pred_switching_loss = pred[0]\n",
        "\n",
        "            # Costo multivariable: 2 * THD + switching_loss (refleja prioridad en THD)\n",
        "            # Ensure operations on predictions are with tensors if needed for gradients\n",
        "            pred_thd_tensor = tf.convert_to_tensor(pred_thd, dtype=tf.float32)\n",
        "            pred_switching_loss_tensor = tf.convert_to_tensor(pred_switching_loss, dtype=tf.float32)\n",
        "            cost = 2.0 * pred_thd_tensor + pred_switching_loss_tensor\n",
        "\n",
        "        # Gradientes respecto a lambdas\n",
        "        grads = tape.gradient(cost, lambdas)\n",
        "        optimizer.apply_gradients(zip([grads], [lambdas]))\n",
        "\n",
        "        # Clamp lambdas a [0, 5]\n",
        "        lambdas.assign(tf.clip_by_value(lambdas, 0.0, 5.0))\n",
        "\n",
        "    return lambdas.numpy(), cost.numpy() # Return cost as numpy as well\n",
        "\n",
        "# Optimizar para 10 ejemplos aleatorios del dataset de evaluación\n",
        "eval_sample = eval_df.sample(10, random_state=42)\n",
        "for idx, row in eval_sample.iterrows():\n",
        "    fixed_conditions = row[[\"num_cells\", \"Vdc\", \"C_dc\", \"fsw\", \"R_load\", \"L_load\", \"mod_index\"]].values\n",
        "    fixed_conditions_tf = tf.convert_to_tensor(fixed_conditions, dtype=tf.float32)\n",
        "\n",
        "    opt_lambdas, opt_cost = optimize_lambdas(fixed_conditions_tf, model, scaler_X, scaler_y)\n",
        "    original_thd = row[\"THD\"]\n",
        "    original_switching_loss = row[\"switching_loss\"]\n",
        "    original_cost = 2.0 * original_thd + original_switching_loss\n",
        "\n",
        "    print(f\"Ejemplo {idx}:\")\n",
        "    print(f\"  Condiciones: {fixed_conditions}\")\n",
        "    print(f\"  Lambdas originales: λ_i={row['lambda_i']:.3f}, λ_v={row['lambda_v']:.3f}, λ_sw={row['lambda_sw']:.3f} | Costo original: {original_cost:.3f}\")\n",
        "    print(f\"  Lambdas optimizados: λ_i={opt_lambdas[0]:.3f}, λ_v={opt_lambdas[1]:.3f}, λ_sw={opt_lambdas[2]:.3f} | Costo optimizado: {opt_cost:.3f}\\n\")\n",
        "\n",
        "# Guardar el modelo para uso futuro\n",
        "model.save(\"chb_surrogate_model.h5\")\n",
        "print(\"Modelo guardado como 'chb_surrogate_model.h5'\")"
      ]
    }
  ]
}